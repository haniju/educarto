# -*- coding: utf-8 -*-
"""
Created on Fri Dec 19 11:27:12 2014

@author: Noëline LEVI ALVARES
@licence:
 This program is free software: you can redistribute it and/or modify
 it under the terms of the GNU Affero General Public License as published by
 the Free Software Foundation, either version 3 of the License, or
 (at your option) any later version.
 
 This program is distributed in the hope that it will be useful,
 but WITHOUT ANY WARRANTY; without even the implied warranty of
 MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
 GNU Affero General Public License for more details.
 
 You should have received a copy of the GNU General Public License
 along with this program.  If not, see <http://www.gnu.org/licenses/>.
"""
import csv
from collections import defaultdict
from math import log

import xml.etree.ElementTree as ET
tree = ET.parse('halXMLmooc.xml')
root = tree.getroot()


def categorie(t):
    if (t.attrib == {'name': 'title_s'}):
        result = "title"
    elif (t.attrib == {'name': 'abstract_s'}):
        result = "des"
    elif (t.attrib == {'name': 'language_s'}):
        result = "lg"
    elif (t.attrib == {'name': 'authFullName_s'}):
        result = "author"
    elif (t.attrib == {'name': 'dateLastIndexed_tdate'}):
        result = "year"
    else:
        result = 0
    return result


def getInfo(t):
    categ = categorie(t)
    if (categ == 'year'):
        contenu = t.text[0:10]
    else:
        contenu = ""
        for child in t.findall('str'):
            contenu += (child.text)
            contenu += (" ")
    return ([categ, contenu])

# print(root[0][3][3].attrib)
# print(getInfo(root[0][5][2]))

listR = root[0]  # correspond au tag "result"
listG = []

for child in listR:
    if (child.tag == 'doc'):
        listC = []
        for item in child:
            if categorie(item) != 0:
                listC.append(getInfo(item))
        listG.append(listC)

# print("\n", "Liste générale:", listG)


file = open("articlesHal.csv", "w")
fileW = csv.writer(file, lineterminator='\n')
fileW.writerow(["Index","Titre","Auteur","Description","Année de publication","Langue"])

n = 1
for l in listG:
    title = ""
    author = ""
    des = ""
    year = ""
    lg = ""
    for tup in l:
        if tup[0] == 'title':
            title = tup[1]
            # print("titre: ", title)
        elif tup[0] == 'author':
            author = tup[1]
            # print("author: ", author)
        elif tup[0] == 'des':
            des = tup[1]
            # print("Description: ", des)
        elif tup[0] == 'year':
            year = tup[1]
            # print("Année: ", year)
        elif tup[0] == 'lg':
            lg = tup[1][0:2]  # on enlève l'espace en trop après la lg
            # print("Langue: ", lg)
    if (lg == 'eng') or (lg == 'en'):
        print([n,title,author,des,year,lg])
        fileW.writerow([n,title,author,des,year,lg])
        n += 1
file.close()


#  création d'1 liste de listes des descriptions et leur index inscrites ds le fichier csv
fileR = open("articlesHal.csv", "r")
fileRR = csv.reader(fileR)
nb_doc = 0
description_IndexList = []
for row in fileRR:
    if (len(row) > 0):  # rajouter (and len(row)!= ...) si on veut enlever ctn articles
        if (row[3] != ''):
            # print("Nouvel des : ",row[3])
            description_IndexList.append([row[3], nb_doc])
    nb_doc += 1
# print("Final string: ", description_IndexList)
fileR.close()


#  #enlever ponctuation
import string
punct = set(string.punctuation)
punct.add('"')
# print(punct)

#  création d'1 liste de listes des descriptions sans ponctuation et leur index
desInd_sansPunct = []
for tup in description_IndexList:
    for i in punct:
        tup[0] = tup[0].replace(i, '')
    desInd_sansPunct.append([tup[0].split(), tup[1]])
# print("Liste de liste des mots des différentes descriptions : ", desInd_sansPunct)

#  # Att : enlever research, result, also, overall
import nltk
from nltk.corpus import stopwords

stopW = set(stopwords.words('english'))
index = 0
wordsLLF = []  # liste de listes des mots de chaque descriptions
finalWordList = []  # liste  globale des mots ac doublons de toutes les descriptions
table_MotIndex = []  # table comportant cq mot (même doublons) ac l'index du document duquel il provient
for tup in desInd_sansPunct:
    newliste = []
    for word in tup[0]:
        if (word.lower() not in stopW):   # on supprime les mots ordinaires
            if(word.lower not in newliste):  #on supprime les doublons ds wordsLLF
                newliste.append(word.lower())  # on met tt en minuscule
                table_MotIndex.append([word.lower(), tup[1]])
            finalWordList = finalWordList + [word.lower()]
    wordsLLF.append(newliste)
    # finalWordList = finalWordList + newliste
# print("Liste finale : ", finalWordList)
# print("Liste de liste finale: ", wordsLLF)
# print("Liste index: ", table_MotIndex)

#  #création dico ac nb occurences de cq mot
ocL = {}.fromkeys(set(finalWordList), 0)
for valeur in finalWordList:
    ocL[valeur] += 1
# print(ocL)


#link_list = [[mooc,study][mooc,participant]...] => sert à former liens ds Gephi
link_list = []
# création matrix de coocurences de chaque paires de mots
def cooccurence_matrix_corpus(corpus,liste):
    # corpus = liste de liste de mots contenu ds 1 description --> faire cette liste!
    matrix = defaultdict(lambda: defaultdict(int))
    for description in corpus:
        for i in range(len(description)-1):
            for j in range(i+1, len(description)):
                word1 = description[i]
                word2 = description[j]
                liste.append([word1, word2])
                matrix[word1][word2] += 1
                matrix[word2][word1] += 1
    return matrix

co_ocM = cooccurence_matrix_corpus(wordsLLF, link_list)
# print(co_ocM)


fileL = open("linksHal.csv", "w")
fileLW = csv.writer(fileL, lineterminator='\n')  # ne pas oublier lineterminator, sans quoi on a des lignes vides
fileLW.writerow(["Source", "Target"])
for tup in link_list:
    fileLW.writerow([tup[0], tup[1]])
fileL.close()


# création du dico de coocurence de chaque mot (moyenne des coocurences d'avec les autres mots)
co_ocL = {}.fromkeys(set(finalWordList), 0)
for valeur in set(finalWordList):
    nb_w = len(set(finalWordList))
    somme = 0
    for key, value in co_ocM[valeur].items():
        somme += value
    if nb_w != 0:
        co_ocL[valeur] = somme/nb_w
# print(co_ocL)


def iJacquart_matrix_corpus(corpus):
    # corpus = liste globale de tous les mots (sans doublons)
    matrix = defaultdict(lambda: defaultdict(float))
    for i in range(len(corpus)-1):
        for j in range(i+1, len(corpus)):
            word1 = corpus[i]
            word2 = corpus[j]
            co_ocr = co_ocM[word1][word2]
            sum_ocr = ocL[word1] + ocL[word2]
            matrix[word1][word2] = co_ocr/sum_ocr
            matrix[word2][word1] = co_ocr/sum_ocr
    return matrix

iJM = iJacquart_matrix_corpus(list(set(finalWordList)))

# création du dico de l'indice de Jacquart de chaque mot
iJL = {}.fromkeys(set(finalWordList), 0)
for valeur in set(finalWordList):
    nb_w = len(set(finalWordList))
    somme = 0
    for key, value in iJM[valeur].items():
        somme += value
    if nb_w != 0:
        iJL[valeur] = somme/nb_w
# print(iJL)


# renvoie un dico contenant le nb de doc contenant le mot word
def nombre_doc_contenant(word, corpus):
    n = 0
    for liste in corpus:
        if (word in liste):
            n += 1
    return n

# dico contenant chaque mot avec le nombre de doc le contenant
nb_doc_mot = {}.fromkeys(set(finalWordList), 0)
for word in set(finalWordList):
    nb_doc_mot[word] = nombre_doc_contenant(word, wordsLLF)
# print("Nombre de doc par mots: ", nb_doc_mot)

# dico contenant Inverse document frequency de chaque mot
idf = {}.fromkeys(set(finalWordList), 0)
for word in set(finalWordList):
    idf[word] = log((nb_doc - 1) / nb_doc_mot[word])
# print("idf par mots: ", idf)

tf = {}.fromkeys(set(finalWordList), 0)
for word in set(finalWordList):
    tf[word] = ocL[word]/len(finalWordList)
# print("tf par mots: ", tf)

tfidf = {}.fromkeys(set(finalWordList), 0)
for word in set(finalWordList):
    tfidf[word] = idf[word]*tf[word]
# print("tfidf par mots: ", tf)


#  #création du fichier de statistiques
fileS = open("wordStatHal.csv", "w")
fileSW = csv.writer(fileS, lineterminator='\n')  # ne pas oublier lineterminator, sans quoi on a des lignes vides
fileSW.writerow(["Mot", "Nb occurences", "Nb Co-occurences", "Indice de Jacquart", "Term frequency", "Inverse doc freq", "TF-IDF"])
for word in set(finalWordList):
    fileSW.writerow([word, ocL[word], co_ocL[word], iJL[word], tf[word], idf[word], tfidf[word]])
fileS.close()


#  #création du fichier d'index de chaque mot
fileIndex = open("wordsIndexHal.csv", "w")
fileIndexW = csv.writer(fileIndex, lineterminator='\n')  # ne pas oublier lineterminator, sans quoi on a des lignes vides
fileIndexW.writerow(["Mot", "Index"])
for tup in table_MotIndex:
    fileIndexW.writerow([tup[0], tup[1]])
fileIndex.close()


# création du fichier ordonné des mots selon tfidf
dataF = open('wordStatHal.csv', "r")
data = csv.reader(dataF)
l = 0
sortedlist = []
for row in data:
    if (l == 0):
        header = row
    else:
        line = row
        line[1] = float(line[1])  # mettre int si itemgetter = 1
        sortedlist.append(line)
    l += 1
import operator
sortedlist.sort(key=operator.itemgetter(6), reverse=True)
# print(sortedlist)
dataF.close()


fileOrdered = open("wordsOrdHal.csv", "w")
fileOrderedW = csv.writer(fileOrdered, lineterminator='\n')  # ne pas oublier lineterminator, sans quoi on a des lignes vides
fileOrderedW.writerow(header)
for row in sortedlist:
    fileOrderedW.writerow(row)
fileOrdered.close()

# réduction du fichier ordonné par seulement les 100 mots et l'indice intéressant
j = 0
liste100 = []  # liste des 100 mots retenus
fileOrdered = open("100wordsOrdHal.csv", "w")
fileOrderedW = csv.writer(fileOrdered, lineterminator='\n')  # ne pas oublier lineterminator, sans quoi on a des lignes vides
fileOrderedW.writerow(["Word", "TF-IDF"])
for row in sortedlist:  # on ne prend que 100 mots sur la totalité
    if (j < 100):
        fileOrderedW.writerow([row[0], row[6]])
        liste100 += [row[0]]
    else:
        break
    j += 1
fileOrdered.close()

# récupération des id pour gephi
fileR = open('projet100motsIdHal.csv', "r")
file = csv.reader(fileR)
dicoId = {}.fromkeys(liste100, 0)  # dico contenant les [mot: Id] ac Id donnés par Gephi aux 100 mots
k = 0
for row in file:
    if k != 0:
        dicoId[row[1]] = row[0]
    k += 1
fileR.close()


# création du fichier de coocurences des 100 mots
co_occurenceListe100 = []
for i in range(len(liste100)-1):
        for j in range(i+1, len(liste100)):
            word1 = liste100[i]
            word2 = liste100[j]
            coo = co_ocM[word1][word2]
            co_occurenceListe100.append([dicoId[word1], dicoId[word2], coo])
"""
fileLink = open("100linksHal.csv", "w")
fileLinkW = csv.writer(fileLink, lineterminator='\n')  # ne pas oublier lineterminator, sans quoi on a des lignes vides
fileLinkW.writerow(["Source", "Target", "Co-occurence"])
for row in co_occurenceListe100:
    fileLinkW.writerow(row)
fileLink.close()

"""
# création du fichier de links ac les Id et les coocurences
fileLink = open("gephiLinksHal.csv", "w")
fileLinkW = csv.writer(fileLink, lineterminator='\n')  # ne pas oublier lineterminator, sans quoi on a des lignes vides
fileLinkW.writerow(["Source", "Target", "Co-occurence"])
for row in co_occurenceListe100:
    fileLinkW.writerow(row)
fileLink.close()
